{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['create_image']='True'\n",
    "#os.environ['repository']='romeokienzler'\n",
    "os.environ['install_requirements']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.413514,
     "end_time": "2021-01-28T15:59:34.938111",
     "exception": false,
     "start_time": "2021-01-28T15:59:32.524597",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if bool(os.environ.get('create_image',False)):\n",
    "    docker_file=\"\"\"\n",
    "    FROM registry.access.redhat.com/ubi8/python-38\n",
    "    RUN pip install nvflare==2.0.16\n",
    "    RUN pip install tensorflow==2.9.1\n",
    "    ADD nvflare.ipynb /\n",
    "    ENTRYPOINT [\"ipython\",\"/nvflare.ipynb\"]\n",
    "    \"\"\"\n",
    "    with open(\"Dockerfile\", \"w\") as text_file: \n",
    "        text_file.write(docker_file)\n",
    "\n",
    "    !docker build -t claimed-train-nvflare .\n",
    "    !docker tag claimed-train-nvflare `echo $repository`/claimed-train-nvflare\n",
    "    !docker push `echo $repository`/claimed-train-nvflare\n",
    "elif bool(os.environ.get('install_requirements',False)):\n",
    "    !wget https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install yolo5==0.0.1 torch==1.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import ResNet50V2 # , MobileNetV3Small\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from claimed_utils import unzip, zipdir\n",
    "import os.path\n",
    "import glob\n",
    "from io import BytesIO\n",
    "from minio import Minio\n",
    "from ansible.module_utils.parsing.convert_bool import boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dependency codait_utils.ipynb\n",
    "# @param image_shape shape the images shall be scaled to and the\n",
    "# model then will accept\n",
    "# @param model zip file name\n",
    "# @param data zip file name\n",
    "# @param model folder name\n",
    "# @param data folder name\n",
    "# @param epochs number of epochs to train\n",
    "# @param checkpoint activate checkpointing\n",
    "# @param checkpoint_ip minio endpoint\n",
    "# @param checkpoint_user minio user\n",
    "# @param checkpoint_pass minio pw\n",
    "# @param checkpoint_bucket minio bucket\n",
    "# @returns model zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = os.environ.get('image_shape', '400,400')\n",
    "model_zip = os.environ.get('model_zip', 'model.zip')\n",
    "data_zip = os.environ.get('data_zip', 'data.zip')\n",
    "model_folder = os.environ.get('model', 'model')\n",
    "data = os.environ.get('data', 'data')\n",
    "epochs = int(os.environ.get('epochs', 1))\n",
    "checkpoint = boolean(os.environ.get('checkpoint', False))\n",
    "checkpoint_ip = os.environ.get('checkpoint_ip')\n",
    "checkpoint_user = os.environ.get('checkpoint_user', 'minio')\n",
    "checkpoint_pass = os.environ.get('checkpoint_pass', 'minio123')\n",
    "checkpoint_bucket = os.environ.get('checkpoint_bucket', 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yolov5\n",
    "\n",
    "# model\n",
    "model = yolov5.load('yolov5s')\n",
    "\n",
    "# image\n",
    "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n",
    "\n",
    "# inference\n",
    "results = model(img)\n",
    "\n",
    "# inference with larger input size\n",
    "results = model(img, size=1280)\n",
    "\n",
    "# inference with test time augmentation\n",
    "results = model(img, augment=True)\n",
    "\n",
    "# show results\n",
    "results.show()\n",
    "\n",
    "# save results\n",
    "results.save(save_dir='results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yolov5\n",
    "\n",
    "# model\n",
    "model = yolov5.load('yolov5s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DetectMultiBackend' object has no attribute 'input_details'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/elyra/lib64/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/elyra/lib64/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/elyra/lib64/python3.9/site-packages/yolov5/models/common.py:581\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, imgs, size, augment, profile)\u001b[0m\n\u001b[1;32m    577\u001b[0m t\u001b[38;5;241m.\u001b[39mappend(time_sync())\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(autocast):\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     t\u001b[38;5;241m.\u001b[39mappend(time_sync())\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# Post-process\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/elyra/lib64/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/elyra/lib64/python3.9/site-packages/yolov5/models/common.py:469\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize, val)\u001b[0m\n\u001b[1;32m    467\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrozen_func(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf\u001b[38;5;241m.\u001b[39mconstant(im))\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Lite or Edge TPU\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_details\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_details[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    470\u001b[0m     int8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39muint8  \u001b[38;5;66;03m# is TFLite quantized uint8 model\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m int8:\n",
      "File \u001b[0;32m~/venvs/elyra/lib64/python3.9/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DetectMultiBackend' object has no attribute 'input_details'"
     ]
    }
   ],
   "source": [
    "# image\n",
    "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n",
    "\n",
    "# inference\n",
    "results = model(img, augment=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov5 import YOLOv5\n",
    "import torch \n",
    "\n",
    "torch.load(map_location=torch.device('cpu'))\n",
    "# set model params\n",
    "model_path = \"yolov5/weights/yolov5s.pt\" # it automatically downloads yolov5s model to given path\n",
    "device = \"cuda\" # or \"cpu\"\n",
    "\n",
    "# init yolov5 model\n",
    "yolov5 = YOLOv5(model_path, device)\n",
    "\n",
    "# load images\n",
    "image1 = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n",
    "image2 = 'https://github.com/ultralytics/yolov5/blob/master/data/images/bus.jpg'\n",
    "\n",
    "# perform inference\n",
    "results = yolov5.predict(image1)\n",
    "\n",
    "# perform inference with larger input size\n",
    "results = yolov5.predict(image1, size=1280)\n",
    "\n",
    "# perform inference with test time augmentation\n",
    "results = yolov5.predict(image1, augment=True)\n",
    "\n",
    "# perform inference on multiple images\n",
    "results = yolov5.predict([image1, image2], size=1280, augment=True)\n",
    "\n",
    "# show detection bounding boxes on image\n",
    "results.show()\n",
    "\n",
    "# save results into \"results/\" folder\n",
    "results.save(save_dir='results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.042719,
   "end_time": "2021-01-28T16:00:26.871724",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "output_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T15:59:31.829005",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
