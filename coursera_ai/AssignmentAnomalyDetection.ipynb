{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Programming Assignment\n",
    "\n",
    "In this assignment, you will implement re-use the unsupervised anomaly detection algorithm but turn it into a simpler feed forward neural network for supervised classification.\n",
    "\n",
    "You are training the neural network from healthy and broken samples and at later stage hook it up to a message queue for real-time anomaly detection.\n",
    "\n",
    "We've provided a skeleton for you containing all the necessary code but left out some important parts indicated with ### your code here ###\n",
    "\n",
    "After you’ve completed the implementation please submit it to the autograder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports\n",
    "\n",
    "try to import all necessary 3rd party packages which are not already part of the default data science experience installation\n",
    "\n",
    "note: If you are running outside data science experience you might need a \"!pip install pandas scikit-learn tensorflow\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "\n",
    "try:\n",
    "    __import__('keras')\n",
    "except ImportError:\n",
    "    pip.main(['install', 'keras']) \n",
    "    \n",
    "try:\n",
    "    __import__('ibmiotf')\n",
    "except ImportError:\n",
    "    pip.main(['install', 'ibmiotf']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import all the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ibmiotf.application\n",
    "import sys\n",
    "from queue import Queue\n",
    "import pandas as pd\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the files necessary for taining. Those are sampled from the lorenz attractor model implemented in NodeRED. Those are two serialized pickle numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm watsoniotp.*\n",
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.healthy.phase_aligned.pickle\n",
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.broken.phase_aligned.pickle\n",
    "!mv watsoniotp.healthy.phase_aligned.pickle watsoniotp.healthy.pickle\n",
    "!mv watsoniotp.broken.phase_aligned.pickle watsoniotp.broken.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De-serialize the numpy array containing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy = pickle.load(open('watsoniotp.healthy.pickle', 'rb'), encoding='latin1')\n",
    "data_broken = pickle.load(open('watsoniotp.broken.pickle', 'rb'), encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to three columns and 3000 rows. In other words three vibration sensor axes and 3000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy = data_healthy.reshape(3000,3)\n",
    "data_broken = data_broken.reshape(3000,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is sampled from the Lorenz Attractor Model, let's plot it with a phase lot to get the typical 2-eyed plot. First for the healthy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot(data_healthy[:,0], data_healthy[:,1], data_healthy[:,2],lw=0.5)\n",
    "ax.set_xlabel(\"X Axis\")\n",
    "ax.set_ylabel(\"Y Axis\")\n",
    "ax.set_zlabel(\"Z Axis\")\n",
    "ax.set_title(\"Lorenz Attractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the broken one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot(data_broken[:,0], data_broken[:,1], data_broken[:,2],lw=0.5)\n",
    "ax.set_xlabel(\"X Axis\")\n",
    "ax.set_ylabel(\"Y Axis\")\n",
    "ax.set_zlabel(\"Z Axis\")\n",
    "ax.set_title(\"Lorenz Attractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples, we fed the raw data into an LSTM. Now we want to use an ordinary feed-forward network. So we need to do some pre-processing of this time series data\n",
    "\n",
    "A widely-used method in traditional data science and signal processing is called Discrete Fourier Transformation. This algorithm transforms from the time to the frequency domain, or in other words, it returns the frequency spectrum of the signals.\n",
    "\n",
    "The most widely used implementation of the transformation is called FFT, which stands for Fast Fourier Transformation, let’s run it and see what it returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy_fft = np.fft.fft(data_healthy)\n",
    "data_broken_fft = np.fft.fft(data_broken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first have a look at the shape and contents of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data_healthy_fft.shape)\n",
    "print (data_healthy_fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we notice that the shape is the same as the input data. So if we have 3000 samples, we get back 3000 spectrum values, or in other words 3000 frequency bands with the intensities.\n",
    "\n",
    "The second thing we notice is that the data type of the array entries is not float anymore, it is complex. So those are not complex numbers, it is just a means for the algorithm the return two different frequency compositions in one go. The real part returns a sine decomposition and the imaginary part a cosine. We will ignore the cosine part in this example since it turns out that the sine part already gives us enough information to implement a good classifier.\n",
    "\n",
    "But first let’s plot the two arrays to get an idea how a healthy and broken frequency spectrum differ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(data_healthy_fft)\n",
    "ax.plot(range(0,size), data_healthy_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_healthy_fft[:,1].real, '-', color='red', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_healthy_fft[:,2].real, '-', color='green', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(data_healthy_fft)\n",
    "ax.plot(range(0,size), data_broken_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_broken_fft[:,1].real, '-', color='red', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_broken_fft[:,2].real, '-', color='green', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we've been doing is so called feature transformation step. We’ve transformed the data set in a way that our machine learning algorithm – a deep feed forward neural network implemented as binary classifier – works better. So now let's scale the data to a 0..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And please don’t worry about the warnings. As explained before we don’t need the imaginary part of the FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy_scaled = scaleData(data_healthy_fft)\n",
    "data_broken_scaled = scaleData(data_broken_fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reshape again to have three examples (rows) and 3000 features (columns). It's important that you understand this. We have turned our initial data set which containd 3 columns (dimensions) of 3000 samples. Since we applied FFT on each column we've obtained 3000 spectrum values for each of the 3 three columns. We are now using each column with the 3000 spectrum values as one row (training example) and each of the 3000 spectrum values becomes a column (or feature) in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy_scaled.shape = (3, 3000)\n",
    "data_broken_scaled.shape = (3, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Assignment\n",
    "\n",
    "The first thing we need to do is to install a little helper library for submitting the solutions to the coursera grader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f rklib.py\n",
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify you email address you are using with cousera here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rklib import submit, submitAll\n",
    "key = \"4vkB9vnrEee8zg4u9l99rA\"\n",
    "all_parts = [\"O5cR9\",\"0dXlH\",\"ZzEP8\"]\n",
    "\n",
    "email = \"###_YOUR_CODE_GOES_HERE_###\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task\n",
    "\n",
    "Given, the explanation above, please fill in the following two constants in order to make the neural network work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ###\n",
    "dim = #### your code here ###\n",
    "samples = #### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Now it’s time to submit your first solution. Please make sure that the secret variable contains a valid submission token. You can obtain it from the courser web page of the course using the grader section of this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = \"O5cR9\"\n",
    "secret = \"###_YOUR_CODE_GOES_HERE_###\"\n",
    "\n",
    "submitAll(email, secret, key, dict((p, json.dumps({}) if p != part else json.dumps({\"dim\": dim, \"samples\": samples})) for p in all_parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe how training works we just print the loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        sys.stdout.write(str(logs.get('loss'))+str(', '))\n",
    "        sys.stdout.flush()\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "lr = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Please fill in the following constants to properly configure the neural network. For some of them you have to find out the precise value, for others you can try and see how the neural network is performing at a later stage. The grader only looks at the values which need to be precise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_neurons_layer1 = #### your code here ###\n",
    "number_of_neurons_layer2 = #### your code here ###\n",
    "number_of_neurons_layer3 = #### your code here ###\n",
    "number_of_epochs = #### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Please submit your constants to the grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_data = {}\n",
    "parts_data[\"0dXlH\"] = json.dumps({\"number_of_neurons_layer1\": number_of_neurons_layer1, \"number_of_neurons_layer2\": number_of_neurons_layer2, \"number_of_neurons_layer3\": number_of_neurons_layer3, \"number_of_epochs\": number_of_epochs})\n",
    "parts_data[\"O5cR9\"] = json.dumps({\"dim\": dim, \"samples\": samples})\n",
    "parts_data[\"ZzEP8\"] = None \n",
    "                                 \n",
    "                                 \n",
    "secret = \"###_YOUR_CODE_GOES_HERE_###\"\n",
    "\n",
    "\n",
    "submitAll(email, secret, key, parts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Now it’s time to create the model. Please fill in the placeholders. Please note since this is only a toy example, re don't use a separate corpus for training and testing. Just use the same data for fitting and scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_of_neurons_layer1,input_shape=(dim, ), activation='relu'))\n",
    "model.add(Dense(number_of_neurons_layer2, activation='relu'))\n",
    "model.add(Dense(number_of_neurons_layer3, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "def train(data,label):\n",
    "    model.fit(#### your code here ###, #### your code here ###, epochs=number_of_epochs, batch_size=72, validation_data=(data, label), verbose=0, shuffle=True,callbacks=[lr])\n",
    "\n",
    "def score(data):\n",
    "    return model.#### your code here ###(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some learners constantly reported 502 errors in Watson Studio. \n",
    "#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n",
    "#This is a workaround to limit resource consumption\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the training data by concatenating a label “0” for the broken and a label “1” for the healthy data. Finally we union the two data sets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_healthy = np.repeat(1,3)\n",
    "label_healthy.shape = (3,1)\n",
    "label_broken = np.repeat(0,3)\n",
    "label_broken.shape = (3,1)\n",
    "\n",
    "train_healthy = np.hstack((data_healthy_scaled,label_healthy))\n",
    "train_broken = np.hstack((data_broken_scaled,label_broken))\n",
    "train_both = np.vstack((train_healthy,train_broken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the two training sets for broken and healthy and at the union of them. Note that the last column is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those are frequency bands. Notice that although many frequency bands are having nearly the same energy, the neural network algorithm still can work those out which are significantly different. \n",
    "\n",
    "## Task\n",
    "\n",
    "Now it’s time to do the training. Please provide the first 3000 columns of the array as the 1st parameter and column number 3000 containing the label as 2nd parameter. Please use the python array slicing syntax to obtain those. \n",
    "\n",
    "The following link tells you more about the numpy array slicing syntax\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_both[:,#### your code here ###]\n",
    "labels = train_both[:,#### your code here ###]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to do the training. You should see the loss trajectory go down, we will also plot it later. Note: We also could use TensorBoard for this but for this simple scenario we skip it. In some rare cases training doesn’t converge simply because random initialization of the weights caused gradient descent to start at a sub-optimal spot on the cost hyperplane. Just recreate the model (the cell which contains *model = Sequential()*) and re-run all subsequent steps and train again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(lr.losses)\n",
    "ax.plot(range(0,size), lr.losses, '-', color='blue', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s examine whether we are getting good results. Note: best practice is to use a training and a test data set for this which we’ve omitted here for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(data_healthy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(data_broken_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task\n",
    "\n",
    "Now it’s time to hook this up to the message queue for real-time data analysis. Please provide your credentials here. You’ve learned how to obtained them in the 1st week of the course in the lecture called “Setup the NodeRED Boilerplate”, at the end of the video. Of course, you need to install your own instance in the IBM Cloud.\n",
    "\n",
    "IMPORTANT NOTE: In case you haven't setup the TestData Generator, please do so. You can watch me doing this in the video \"Setup the NodeRED Boilerplate\" of Week 1. The important steps are shown around 2 minutes 30 seconds.\n",
    "\n",
    "If you want to learn more about the physical model and the simulator as a whole, I've documented this here:\n",
    "\n",
    "https://www.ibm.com/developerworks/analytics/library/iot-deep-learning-anomaly-detection-2/index.html?ca=drs-\n",
    "\n",
    "BTW: This article is part of a 5 part series on anomaly detection I've written last year :)\n",
    "\n",
    "If you just need the JSON of the NodeRED flow, you can find it here:\n",
    "\n",
    "https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/simulatorflow.json\n",
    "\n",
    "I'm deeply thankful to Nicole Finnie, one of the learners, to point out this missing information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"org\": \"#### your code here ###\", \"id\": \"anything\", \"auth-method\": \"apikey\", \"auth-key\": \"#### your code here ###\", \"auth-token\": \"#### your code here ###\"}\n",
    "client = ibmiotf.application.Client(options)\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a python queue for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue(7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to real time data, we need to specify a callback handler where we extract and reformat the data, register the callback handler to the MQTT client and select the type of events we want to subscribe to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myEventCallback(event):\n",
    "    sample = event.data\n",
    "    point = [sample[\"x\"], sample[\"y\"],sample[\"z\"]]\n",
    "    q.put(point)\n",
    "\n",
    "client.deviceEventCallback = myEventCallback\n",
    "client.subscribeToDeviceEvents(\"0.16.2\", \"lorenz\", \"osc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can call the neural network scoring (or predicting) function we need to pre-proceed the raw data in the same way as we have done for the static data we’ve used for training. Note that we take the average score over the three samples. Finally, we push the result back the message queue.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "In case you feel confident that everything works as it should you can set *submit_work* to True and again please make sure that the secret variable contains a valid submission token. Please make sure to re-run the whole scenario end-2-end from the TestDataGenerator in NodeRED so that the grader can pick up your result based on the live stream of data.\n",
    "\n",
    "Please open NodeRED and click on “reset” on the test data generator. On the debug tab you should see the data generated and in the notebook you should see little dots apprearing, one for each message received. Once 3000 messages have been received, the data is sent downstream to your neural network and once it has finished the anomaly score will be sent to the grader for grading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_work = False ###_YOUR_CODE_GOES_HERE_### => set to True in case you want to submit to the grader\n",
    "parts_data = {}\n",
    "parts_data[\"0dXlH\"] = json.dumps({\"number_of_neurons_layer1\": number_of_neurons_layer1, \"number_of_neurons_layer2\": number_of_neurons_layer2, \"number_of_neurons_layer3\": number_of_neurons_layer3, \"number_of_epochs\": number_of_epochs})\n",
    "parts_data[\"O5cR9\"] = json.dumps({\"dim\": dim, \"samples\": samples})\n",
    "\n",
    "                                 \n",
    "                                 \n",
    "secret = \"###_YOUR_CODE_GOES_HERE_###\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doNN(data):\n",
    "    global submit_work\n",
    "    data_fft = np.fft.fft(data)\n",
    "    data_scaled = scaleData(data_fft)\n",
    "    data_scaled_reshaped = data_scaled\n",
    "    data_scaled_reshaped.shape = (3, 3000)\n",
    "    prediction = str(np.sum(score(data_scaled_reshaped))/3)\n",
    "    print (\"Prediction: %s, publishing...\" % (prediction))\n",
    "    myData={'healthy' : prediction}\n",
    "    client.publishEvent(\"0.16.2\", \"lorenz\", \"status\", \"json\", myData)\n",
    "    if submit_work:\n",
    "        submit_work = False\n",
    "        parts_data[\"ZzEP8\"] = json.dumps(myData)\n",
    "        submitAll(email, secret, key, parts_data)\n",
    "        print (\"Submitting to grader: %s\" % (json.dumps(myData)))\n",
    "    print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to start the endless loop to wait for data arriving in the queue. Once started please start generating data using the NodeRED test data generator. \n",
    "\n",
    "IMPORTANT: You are blocking the kernel now, so the only way to escape is to click on “Kernel->Restart & Clear Output” and wait until all outputs disappear. Then you can start over from scratch. \n",
    "\n",
    "You can also choose “Cell->Run all”\n",
    "\n",
    "Rarely you might get an “memory error”, in case too many students on the free tier are pushing load to the underlying Kubernetes engine. So, in that case just start over as described above by restarting the kernel and run all again WITHOUT using the “Run all” method, just run each cell, one by one and wait until it has completed (by waiting for the asterisk being replaced by a number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "while True:\n",
    "    while not q.empty():\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "        point = q.get()\n",
    "        try:\n",
    "            data\n",
    "        except NameError:\n",
    "            data = np.array(point)\n",
    "        else:\n",
    "            data = np.append(data,point)\n",
    "        if data.size>=9000:\n",
    "            data = np.reshape(data,(3000,3))\n",
    "            print (\"Sending window downstream to the neural network...\")\n",
    "            doNN(data)\n",
    "            print (\"Training finished...\")\n",
    "            del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get some reshaping errors, just execute the following line and start over with the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
